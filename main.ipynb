{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Organize the Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as u\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section for getting the bonafide files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if len(sys.argv) != 4:\n",
    "    print(\"Usage: python main.py <dataset_directory> <timestamp_file> <output_directory>\")\n",
    "    sys.exit(1)\n",
    "\n",
    "dataset_directory = sys.argv[1]\n",
    "timestamp_file = sys.argv[2]\n",
    "output_directory = sys.argv[3]\n",
    "\n",
    "u.process_dataset(dataset_directory, timestamp_file, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to convert again in the future, just use the files in wav_files folder directly\n",
    "#data_home = '/Volumes/T7/group5_DL4M/dev_set'\n",
    "#u.convert_flac_to_wav_librosa(data_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_directory = '/Users/mona/Documents/GitHub/group_5_DL4M1/new_data/japanese_full_traning_datasets'\n",
    "label_file = '/Users/mona/Documents/GitHub/group_5_DL4M1/new_data/train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/mona/Documents/GitHub/group_5_DL4M1/new_data/japanese_full_traning_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#is there a way to not having to load data and labels everytime?\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m audio_data, audio_labels \u001b[38;5;241m=\u001b[39m \u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/group_5_DL4M1/utils.py:58\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(directory, label_file)\u001b[0m\n\u001b[1;32m     55\u001b[0m audio_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Iterate over audio files in the directory\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.flac\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;66;03m# Load audio file\u001b[39;00m\n\u001b[1;32m     61\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, file_name)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/mona/Documents/GitHub/group_5_DL4M1/new_data/japanese_full_traning_datasets'"
     ]
    }
   ],
   "source": [
    "#is there a way to not having to load data and labels everytime?\n",
    "audio_data, audio_labels = u.load_data(wav_directory, label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example of using loaded data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43maudio_data\u001b[49m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio files.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample labels:\u001b[39m\u001b[38;5;124m\"\u001b[39m, audio_labels[:\u001b[38;5;241m10\u001b[39m]) \u001b[38;5;66;03m# Print the first 10 labels\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'audio_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Example of using loaded data\n",
    "print(\"Loaded\", len(audio_data), \"audio files.\")\n",
    "print(\"Sample labels:\", audio_labels[:10]) # Print the first 10 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Japanese Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dev set\n",
    "wav_directory = '/teamspace/studios/this_studio/data/japanese_dev/japanese_dev'\n",
    "label_file = '/teamspace/studios/this_studio/data/japanese_dev/japanese_dev.txt'\n",
    "u.copy_selected_flac_files(wav_directory, label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_directory = '/teamspace/studios/this_studio/data/japanese_full_train_set_flac2.0/japanese_train_set_flac'\n",
    "label_file = '/teamspace/studios/this_studio/data/train.txt'\n",
    "_, train_labels = u.load_flac_data(wav_directory, label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/teamspace/studios/this_studio/jap_aug_train_labels.npy', train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '/teamspace/studios/this_studio/jap_aug_train_labels.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in train_labels:\n",
    "        f.write(f'{label}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('/teamspace/studios/this_studio/data/dev_bonafide_audio/japanese_full_train_set_flac/japanese_train_set_flac')\n",
    "    \n",
    "    # Count the number of files\n",
    "num_files = len(files)\n",
    "print(num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('/teamspace/studios/this_studio/data/japanese_full_train_set_flac2.0/japanese_train_set_flac')\n",
    "    \n",
    "    # Count the number of files\n",
    "num_files = len(files)\n",
    "print(num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing your audio files\n",
    "directory = '/teamspace/studios/this_studio/data/dev_bonafide_audio/japanese_full_train_set_flac/japanese_train_set_flac'\n",
    "\n",
    "# Initialize a list to store bonafide labels\n",
    "bonafide_labels = []\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Iterate over the files\n",
    "for file_name in files:\n",
    "    # Check if the file name starts with \"bonafide\"\n",
    "    if file_name.startswith('bonafide'):\n",
    "        # Add the label \"bonafide\" to the bonafide_labels list\n",
    "        bonafide_labels.append('bonafide')\n",
    "\n",
    "print(len(bonafide_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_bonafide_audio = \"/teamspace/studios/this_studio/data/japanese_augmented_bonafide\"\n",
    "bonafide_labels = u.create_bonafide_labels(augmented_bonafide_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bonafide_labels[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.extend(bonafide_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"/teamspace/studios/this_studio/data/japanese_augmented_bonafide\"\n",
    "target_dir = \"/teamspace/studios/this_studio/data/dev_bonafide_audio/japanese_full_train_set_flac/japanese_train_set_flac\"\n",
    "\n",
    "# Append directories and labels\n",
    "appended_labels = u.append_directories(source_dir, target_dir, bonafide_labels, train_labels)\n",
    "print(appended_labels[3000:3010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_label_distribution(dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('/teamspace/studios/this_studio/data/dev_bonafide_audio/japanese_full_train_set_flac/japanese_train_set_flac')\n",
    "num_files = len(files)\n",
    "print(num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loaded\", len(train_audio_data), \"audio files.\")\n",
    "print(\"Sample labels:\", train_labels[300:310]) # Print the first 10 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_label_distribution(concatenated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loaded\", len(concatenated_labels), \"labels.\")\n",
    "print(\"Sample labels:\", concatenated_labels[5000:5050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def save_data_and_labels(concatenated_data, concatenated_labels, data_file, labels_file):\n",
    "    # Write concatenated data to CSV\n",
    "    with open(data_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(concatenated_data)\n",
    "\n",
    "    # Write concatenated labels to CSV\n",
    "    with open(labels_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(concatenated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"/teamspace/studios/this_studio/data/japanese_augmented_bonafide\"\n",
    "target_dir = \"/teamspace/studios/this_studio/data/dev_bonafide_audio/japanese_full_train_set_flac/japanese_train_set_flac\"\n",
    "labels_file = \"/path/to/labels.txt\"\n",
    "\n",
    "# Append audio files and generate labels\n",
    "append_audio_files(source_dir, target_dir, labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"/teamspace/studios/this_studio/data/japanese_augmented_bonafide\"\n",
    "concatenated_data, concatenated_labels = u.augment_and_concatenate_bonafide(train_audio_data, train_labels, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = '/teamspace/studios/this_studio/data/japanese_dev/japanese_dev'\n",
    "dev_labels = '/teamspace/studios/this_studio/data/japanese_dev/japanese_dev.txt'\n",
    "dev_audio_data, dev_labels = u.load_flac_data(dev_data, dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loaded\", len(dev_audio_data), \"audio files.\")\n",
    "print(\"Sample labels:\", dev_labels[300:310]) # Print the first 10 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_audio_data, val_labels, test_audio_data, test_labels = u.test_val_split(dev_audio_data, dev_labels, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_label_distribution(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at one sample from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "\n",
    "# Load an audio file\n",
    "file_path = '/Volumes/T7/group5_DL4M/dev_set/wav_files/CtrSVDD_0059_D_0000530.wav'\n",
    "audio, sr = librosa.load(file_path, sr=None)  # Load with the original sample rate\n",
    "\n",
    "# Play the audio\n",
    "display(Audio(data=audio, rate=sr))\n",
    "\n",
    "# Plot the spectrogram. refer: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.colorbar.html\n",
    "plt.figure(figsize=(10, 4))\n",
    "S = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel-frequency spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"sr=\", sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Splite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data, train_labels, val_labels, test_labels = u.split_data(\n",
    "    audio_data, audio_labels, test_size=0.2, val_size=0.2)\n",
    "\n",
    "print(\"Training data size:\", len(train_data))\n",
    "print(\"Validation data size:\", len(val_data))\n",
    "print(\"Test data size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_label_distribution(audio_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_audio_length_distribution(audio_data, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCC & LFCC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the lfcc of one audio file\n",
    "audio, sr = librosa.load('/Users/mona/Documents/GitHub/group_5_DL4M1/new_data/japanese_full_traning_dataset/._CtrSVDD_0001_T_0006771.flac', sr=None)  # Load an audio file\n",
    "mfccs = u.compute_mfcc(audio, sr)\n",
    "print(\"MFCCs:\", mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_mfcc(mfccs, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfccs = u.compute_lfcc(audio, sr)\n",
    "print(\"LFCCs:\", lfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_lfcc(lfccs, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Processing for Feature Extraction\n",
    "# Test \n",
    "test_file_paths = ['/Users/mona/Documents/GitHub/group_5_DL4M1/new_data/japanese_full_traning_dataset/._CtrSVDD_0001_T_0006771.flac',\n",
    "                   '/Users/mona/Documents/GitHub/group_5_DL4M1/new_data/japanese_full_traning_dataset/._CtrSVDD_0001_T_0006772.flac']  # Add your actual paths\n",
    "test_features, test_sr = u.load_and_extract_features(test_file_paths, feature_type='lfcc')\n",
    "\n",
    "print(\"Shape of extracted features:\", test_features.shape)\n",
    "print(\"Sample rate used:\", test_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test storing features in h5 file\n",
    "import h5py\n",
    "\n",
    "# Create an HDF5 file\n",
    "with h5py.File('/Users/mona/Documents/GitHub/group_5_DL4M1/new_data/jap_LFCC_features_test.h5', 'w') as h5f:\n",
    "    # Create a dataset in the file\n",
    "    h5f.create_dataset('LFCC_dataset_test', data=np.array(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_sr = u.load_and_extract_features(test_file_paths, feature_type='mfcc')\n",
    "\n",
    "print(\"Shape of extracted features:\", test_features.shape)\n",
    "print(\"Sample rate used:\", test_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def process_file(file_path):\n",
    "    features, _ = u.load_and_extract_features([file_path], feature_type='lfcc')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = '/Users/mona/Documents/GitHub/group_5_DL4M1/new_data/japanese_full_traning_dataset'\n",
    "\n",
    "#refer to ChatGPT\n",
    "audio_files = [os.path.join(dataset_directory, f) for f in os.listdir(dataset_directory) if f.endswith('.wav') and os.path.isfile(os.path.join(dataset_directory, f))]\n",
    "\n",
    "# with ProcessPoolExecutor() as executor:\n",
    "#     all_features = list(executor.map(process_file, audio_files))\n",
    "\n",
    "# alternative if not using parallel processing\n",
    "\n",
    "all_features = []\n",
    "for file_path in audio_files:\n",
    "    feature = process_file(file_path)\n",
    "    if feature is not None:\n",
    "        all_features.append(feature)\n",
    "\n",
    "# print(\"Features extracted. Number of files processed:\", len(all_features))\n",
    "\n",
    "# Optionally filter out None values and check shapes\n",
    "valid_features = []\n",
    "shapes = set()\n",
    "for feature in all_features:\n",
    "    valid_features.append(feature)\n",
    "    shapes.add(feature.shape)\n",
    "\n",
    "print(\"Unique feature shapes:\", shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corrupted_files(directory):\n",
    "    corrupted_files = []\n",
    "\n",
    "    # List all FLAC files in the directory\n",
    "    flac_files = [f for f in os.listdir(directory) if f.endswith('.flac')]\n",
    "\n",
    "    for file in flac_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        try:\n",
    "            # Try to load the file\n",
    "            sf.read(file_path)\n",
    "        except Exception as e:\n",
    "            # If an exception occurs, the file is likely corrupted\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "            corrupted_files.append(file)\n",
    "\n",
    "    if corrupted_files:\n",
    "        print(\"Corrupted files:\")\n",
    "        for file in corrupted_files:\n",
    "            print(file)\n",
    "    else:\n",
    "        print(\"No corrupted files found.\")\n",
    "\n",
    "# Example usage:\n",
    "check_corrupted_files('/teamspace/studios/this_studio/data/japanese_augmented_bonafide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#where kernel often crashes\n",
    "# Determine the maximum length from the shapes printed\n",
    "max_length = 0\n",
    "for shape in shapes:\n",
    "    length = shape[1]\n",
    "    if length > max_length:\n",
    "\n",
    "        max_length = length\\\n",
    "\n",
    "# Pad all features to the maximum length\n",
    "uniform_features = u.pad_features(all_features, max_length)\n",
    "\n",
    "# Convert list of arrays into a single numpy array for storage\n",
    "uniform_features_array = np.array(uniform_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HDF5 file\n",
    "with h5py.File('/Users/mona/Documents/GitHub/group_5_DL4M1/new_data/jap_LFCC_features_test.h5', 'w') as h5f:\n",
    "    # Create a dataset in the file\n",
    "    h5f.create_dataset('jap_LFCC_dataset_test', data=np.array(uniform_features_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the dataset:\", uniform_features_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_feature, val_data_feature, test_data_feature, train_labels, val_labels, test_labels = u.split_data(\n",
    "    uniform_features_array, audio_labels, test_size=0.2, val_size=0.2)\n",
    "\n",
    "print(\"Training data size:\", len(train_data_feature))\n",
    "print(\"Validation data size:\", len(val_data_feature))\n",
    "print(\"Test data size:\", len(test_data_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train_labels_reshaped = tf.reshape(train_labels, [train_labels.shape[0], 1, 1])\n",
    "val_labels_reshaped = tf.reshape(val_labels, [val_labels.shape[0], 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_feature = train_data_feature[..., np.newaxis]\n",
    "val_data_feature = val_data_feature[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_feature_1 = np.array(train_data_feature, dtype=float)\n",
    "val_data_feature_1 = np.array(val_data_feature, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.where(train_labels == 'deepfake', 1, 0)\n",
    "val_labels = np.where(val_labels == 'deepfake', 1, 0)\n",
    "test_labels = np.where(test_labels == 'deepfake', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data shape:\", train_data_feature_1.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Validation data shape:\", val_data_feature_1.shape)\n",
    "print(\"Validation labels shape:\", val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature array shape:\", uniform_features_array.shape)\n",
    "\n",
    "input_shape = uniform_features_array.shape  \n",
    "print(\"Input shape for model:\", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = uniform_features_array.shape \n",
    "from models import build_model\n",
    "model = build_model(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f\"checkpoints/audio_convnet.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "batch_size = 32\n",
    "history = model.fit(\n",
    "        x=train_data_feature_1,\n",
    "        y=train_labels, \n",
    "        validation_data=(val_data_feature_1, val_labels),\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best checkpoint of the model\n",
    "aug_reloaded = keras.models.load_model(\"checkpoints/audio_aug.keras\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_aug, test_acc_aug = aug_reloaded.evaluate(test_data_feature.batch(batch_size))\n",
    "print(f\"Test Loss: {test_loss_aug:.4f}, Test Accuracy: {test_acc_aug:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YAMNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "yamnet = hub.load('https://tfhub.dev/google/yamnet/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import tcn_model\n",
    "\n",
    "input_shape =  uniform_features_array.shape\n",
    "# Create a tcn model that processes the embeddings\n",
    "tcn_yamnet = tcn_model(input_shape)\n",
    "\n",
    "# Print model summary\n",
    "tcn_yamnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping function to extract embeddings\n",
    "def map_function(audio, label):\n",
    "   return u.extract_yamnet_embedding(audio, yamnet), label\n",
    "\n",
    "# Check input shape from example in the data\n",
    "for e, l in train_data_feature.map(map_function).take(1):\n",
    "    print(e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "model_path = \"checkpoints/yamnet_model.json\"\n",
    "model_weights = \"checkpoints/yamnet_weights.h5\"\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=model_weights,\n",
    "      save_best_only=True,\n",
    "      save_weights_only=True,\n",
    "      monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "history = tcn_yamnet.fit(train_data_feature_1.prefetch(tf.data.AUTOTUNE).map(map_function).batch(batch_size).cache(),\n",
    "    validation_data=val_data_feature_1.prefetch(tf.data.AUTOTUNE).map(map_function).batch(batch_size).cache(),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks)\n",
    "\n",
    "model_as_json = tcn_yamnet.to_json()\n",
    "with open(model_path, \"w\") as json_file:\n",
    "    json_file.write(model_as_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best checkpoint of the model \n",
    "optimizer = 'adam'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "metrics = [\"accuracy\"]\n",
    "tcn_yamnet_reloaded = u.reload_tcn(model_path, model_weights, optimizer, loss, metrics)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_yamnet, test_acc_yamnet = tcn_yamnet_reloaded.evaluate(test_data_feature.prefetch(tf.data.AUTOTUNE).map(map_function).batch(batch_size))\n",
    "print(f\"Test Loss: {test_loss_yamnet:.4f}, Test Accuracy: {test_acc_yamnet:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
