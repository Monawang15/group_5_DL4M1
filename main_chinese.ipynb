{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Organize the Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as u\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to convert again in the future, just use the files in wav_files folder directly\n",
    "data_home = '/Volumes/T7/group5_DL4M/data/dev_set'\n",
    "# u.convert_flac_to_wav_librosa(data_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_directory = '/Volumes/T7/group5_DL4M/dev_set/wav_files'\n",
    "label_file = '/Users/mona/Documents/GitHub/group_5_DL4M1/data/dev.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/T7/group5_DL4M/dev_set/wav_files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m audio_data, audio_labels \u001b[38;5;241m=\u001b[39m \u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/group_5_DL4M1/utils.py:58\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(directory, label_file)\u001b[0m\n\u001b[1;32m     55\u001b[0m audio_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Iterate over audio files in the directory\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;66;03m# Load audio file\u001b[39;00m\n\u001b[1;32m     61\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, file_name)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/T7/group5_DL4M/dev_set/wav_files'"
     ]
    }
   ],
   "source": [
    "audio_data, audio_labels = u.load_data(wav_directory, label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using loaded data\n",
    "print(\"Loaded\", len(audio_data), \"audio files.\")\n",
    "print(\"Sample labels:\", audio_labels[:10]) # Print the first 10 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at one sample from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "\n",
    "# Load an audio file\n",
    "file_path = '/Volumes/T7/group5_DL4M/dev_set/wav_files/CtrSVDD_0059_D_0000530.wav'\n",
    "audio, sr = librosa.load(file_path, sr=None)  # Load with the original sample rate\n",
    "\n",
    "# Play the audio\n",
    "display(Audio(data=audio, rate=sr))\n",
    "\n",
    "# Plot the spectrogram. refer: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.colorbar.html\n",
    "plt.figure(figsize=(10, 4))\n",
    "S = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel-frequency spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"sr=\", sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Splite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data, train_labels, val_labels, test_labels = u.split_data(\n",
    "    audio_data, audio_labels, test_size=0.2, val_size=0.2)\n",
    "\n",
    "print(\"Training data size:\", len(train_data))\n",
    "print(\"Validation data size:\", len(val_data))\n",
    "print(\"Test data size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_label_distribution(audio_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_audio_length_distribution(audio_data, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCC & LFCC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the lfcc of one audio file\n",
    "audio, sr = librosa.load('/Volumes/T7/group5_DL4M/dev_set/wav_files/CtrSVDD_0110_D_0012049.wav', sr=None)  # Load an audio file\n",
    "mfccs = u.compute_mfcc(audio, sr)\n",
    "print(\"MFCCs:\", mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_mfcc(mfccs, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfccs = u.compute_lfcc(audio, sr)\n",
    "print(\"LFCCs:\", lfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_lfcc(lfccs, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Processing for Feature Extraction\n",
    "# Testing on 2 files to see results\n",
    "test_file_paths = ['/Volumes/T7/group5_DL4M/dev_set/wav_files/CtrSVDD_0059_D_0000525.wav',\n",
    "                   '/Volumes/T7/group5_DL4M/dev_set/wav_files/CtrSVDD_0059_D_0000526.wav']  \n",
    "test_features, test_sr = u.load_and_extract_features(test_file_paths, feature_type='lfcc')\n",
    "\n",
    "print(\"Shape of extracted features:\", test_features.shape)\n",
    "print(\"Sample rate used:\", test_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test storing features in h5 file\n",
    "import h5py\n",
    "\n",
    "# Create an HDF5 file\n",
    "with h5py.File('/Users/mona/Documents/GitHub/group_5_DL4M1/data/test_LFCC_features.h5', 'w') as h5f:\n",
    "    h5f.create_dataset('LFCC_dataset_test', data=np.array(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_sr = u.load_and_extract_features(test_file_paths, feature_type='mfcc')\n",
    "\n",
    "print(\"Shape of extracted features:\", test_features.shape)\n",
    "print(\"Sample rate used:\", test_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    features, _ = u.load_and_extract_features([file_path], feature_type='lfcc')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = '/Volumes/T7/group5_DL4M/dev_set/wav_files'\n",
    "\n",
    "# Refer to ChatGPT\n",
    "audio_files = [os.path.join(dataset_directory, f) for f in os.listdir(dataset_directory) if f.endswith('.wav') and os.path.isfile(os.path.join(dataset_directory, f))]\n",
    "\n",
    "# Process for the dataset\n",
    "all_features = []\n",
    "for file_path in audio_files:\n",
    "    feature = process_file(file_path)\n",
    "    if feature is not None:\n",
    "        all_features.append(feature)\n",
    "\n",
    "valid_features = []\n",
    "shapes = set()\n",
    "for feature in all_features:\n",
    "    valid_features.append(feature)\n",
    "    shapes.add(feature.shape)\n",
    "\n",
    "print(\"Unique feature shapes:\", shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum length from the shapes printed\n",
    "max_length = 0\n",
    "for shape in shapes:\n",
    "    length = shape[1]\n",
    "    if length > max_length:\n",
    "\n",
    "        max_length = length\n",
    "\n",
    "# Pad all features to the maximum length\n",
    "uniform_features = u.pad_features(all_features, max_length)\n",
    "\n",
    "# Convert list of arrays into a single numpy array for storage\n",
    "uniform_features_array = np.array(uniform_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HDF5 file\n",
    "with h5py.File('/Users/mona/Documents/GitHub/group_5_DL4M1/data/LFCC_features.h5', 'w') as h5f:\n",
    "    # Create a dataset in the file\n",
    "    h5f.create_dataset('LFCC_dataset_1', data=np.array(uniform_features_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the dataset:\", uniform_features_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data again after feature extraction\n",
    "train_data_feature, val_data_feature, test_data_feature, train_labels, val_labels, test_labels = u.split_data(\n",
    "    uniform_features_array, audio_labels, test_size=0.2, val_size=0.2)\n",
    "\n",
    "print(\"Training data size:\", len(train_data_feature))\n",
    "print(\"Validation data size:\", len(val_data_feature))\n",
    "print(\"Test data size:\", len(test_data_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preperation for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning labels into numpyarray format\n",
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#train_labels_reshaped = tf.reshape(train_labels, [train_labels.shape[0], 1, 1])\n",
    "#val_labels_reshaped = tf.reshape(val_labels, [val_labels.shape[0], 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one more dimension to the dataset shape\n",
    "train_data_feature = train_data_feature[..., np.newaxis]\n",
    "val_data_feature = val_data_feature[..., np.newaxis]\n",
    "test_data_feature = test_data_feature[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn dataset into numpyarray \"float\" format\n",
    "train_data_feature_1 = np.array(train_data_feature, dtype=float)\n",
    "val_data_feature_1 = np.array(val_data_feature, dtype=float)\n",
    "test_data_feautre_1 = np.array(test_data_feature, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn labels into numeric format\n",
    "train_labels = np.where(train_labels == 'deepfake', 1, 0)\n",
    "val_labels = np.where(val_labels == 'deepfake', 1, 0)\n",
    "test_labels = np.where(test_labels == 'deepfake', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of both datasets and labels\n",
    "print(\"Train data shape:\", train_data_feature_1.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Validation data shape:\", val_data_feature_1.shape)\n",
    "print(\"Validation labels shape:\", val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of test dataset & label\n",
    "print(\"Feature array shape:\", uniform_features_array.shape)\n",
    "\n",
    "input_shape = uniform_features_array.shape  \n",
    "print(\"Input shape for model:\", input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = uniform_features_array.shape \n",
    "from models import build_model\n",
    "model = build_model(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f\"checkpoints/audio_convnet.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "batch_size = 32\n",
    "history = model.fit(\n",
    "        x=train_data_feature_1,\n",
    "        y=train_labels, \n",
    "        validation_data=(val_data_feature_1, val_labels),\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best checkpoint of the model\n",
    "model_reloaded = keras.models.load_model(\"checkpoints/audio_convnet.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single batch item from the test dataset and check the accuracy\n",
    "sample_test = test_data_feature[0:1]  \n",
    "try:\n",
    "    result = model_reloaded.predict(sample_test)\n",
    "    print(\"Single sample prediction:\", result)\n",
    "except Exception as e:\n",
    "    print(\"Error during single sample prediction:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reloaded.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('/Users/mona/Documents/GitHub/group_5_DL4M1/checkpoints/audio_convnet.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the h5 file\n",
    "# Refer to https://docs.h5py.org/en/stable/high/file.html\n",
    "import h5py\n",
    "def load_features_h5(file_path):\n",
    "    with h5py.File(file_path, 'r') as hf:\n",
    "        for key in hf.keys():\n",
    "            try:\n",
    "                features = hf[key][:]\n",
    "                return features\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading dataset '{key}': {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "file_path = '/Users/mona/Documents/GitHub/group_5_DL4M1/data/LFCC_features.h5'  \n",
    "features = load_features_h5(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if features is not None:\n",
    "    print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If run through, comment this cell - just for kernal restart\n",
    "import numpy as np\n",
    "test_data_feature = features[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If run through, comment this cell - just for kernal restart\n",
    "test_data_feautre_1 = np.array(test_data_feature, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_feautre_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 40 sample from the dev_set with true labels\n",
    "import utils as u\n",
    "#wav_directory = '/Volumes/T7/group5_DL4M/dev_set/wav_files'\n",
    "wav_directory = '/Users/mona/Documents/GitHub/group_5_DL4M1/predictions audio file'\n",
    "label_file = '/Users/mona/Documents/GitHub/group_5_DL4M1/True Lables Test.txt'\n",
    "_, y_labels = u.load_data(wav_directory, label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils as u\n",
    "predict_directory = '/Users/mona/Documents/GitHub/group_5_DL4M1/predictions audio file'\n",
    "\n",
    "# Refer to ChatGPT\n",
    "audio_files = [os.path.join(predict_directory, f) for f in os.listdir(predict_directory) if f.endswith('.wav') and os.path.isfile(os.path.join(predict_directory, f))]\n",
    "\n",
    "# Alternative if not using parallel processing\n",
    "predict_all_features = []\n",
    "for file_path in audio_files:\n",
    "    feature = process_file(file_path)\n",
    "    if feature is not None:\n",
    "        predict_all_features.append(feature)\n",
    "\n",
    "# Optionally filter out None values and check shapes\n",
    "valid_features = []\n",
    "shapes = set()\n",
    "for feature in predict_all_features:\n",
    "    valid_features.append(feature)\n",
    "    shapes.add(feature.shape)\n",
    "\n",
    "print(\"feature shapes:\", shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad a list of feature arrays to a target length\n",
    "\n",
    "def pad_features(features, target_length):\n",
    "    \"\"\" \n",
    "    Pad the features along the time axis to a target length.\n",
    "    \n",
    "    features (list of np.array): List of feature arrays with shape (13, variable_length).\n",
    "    target_length (int): The target length to pad the time dimension to.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Array of padded features with shape (n_samples, 13, target_length).\n",
    "    \n",
    "    \"\"\"\n",
    "    padded_features = np.zeros((len(features), features[0].shape[0], target_length))\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        sequence_length = feature.shape[1]\n",
    "\n",
    "        if sequence_length > target_length:\n",
    "            padded_features[i, :, :] = feature[:, :target_length]\n",
    "\n",
    "        else:\n",
    "            padded_features[i, :, :sequence_length] = feature\n",
    "    \n",
    "    return padded_features\n",
    "\n",
    "target_length = 798\n",
    "\n",
    "predict_uniform_features_array = pad_features(predict_all_features, target_length)\n",
    "# Add one more dimension to match the shape\n",
    "predict_uniform_features_array = predict_uniform_features_array[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_uniform_features_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(predict_uniform_features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the length of labels and predictions audio are same\n",
    "print(f'Length of y_labels: {len(y_labels)}')\n",
    "print(f'Length of predictions: {len(predictions)}')\n",
    "\n",
    "# If lengths differ, find out if any specific entries are missing\n",
    "if len(y_labels) != len(predictions):\n",
    "    print(\"Mismatch in array lengths detected. Investigating further...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_numeric = np.array([1 if label == 'bonafide' else 0 for label in y_labels])\n",
    "\n",
    "# Then use these numeric labels to calculate EER\n",
    "eer, threshold = u.calculate_eer(y_numeric, predictions)\n",
    "print(f\"EER: {eer:.2%}, Threshold: {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_det_curve(y_numeric, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
